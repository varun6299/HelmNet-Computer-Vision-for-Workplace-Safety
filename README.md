# HelmNet ‚Äî Computer Vision for Workplace Safety

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

*A deep learning solution for automated helmet detection to ensure workplace compliance and safety.*

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

---

## üì¶ Packages Used
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow/Keras](https://img.shields.io/badge/TensorFlow/Keras-DL-orange.svg)](https://www.tensorflow.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-ImageProcessing-green.svg)](https://opencv.org/)
[![NumPy](https://img.shields.io/badge/NumPy-Scientific-blue.svg)](https://numpy.org/)
[![Pandas](https://img.shields.io/badge/Pandas-EDA-teal.svg)](https://pandas.pydata.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-Viz-yellow.svg)](https://matplotlib.org/)
[![Seaborn](https://img.shields.io/badge/Seaborn-StatsViz-lightblue.svg)](https://seaborn.pydata.org/)

---

## Business Context
Workplace safety in hazardous environments like construction sites and industrial plants is critical to prevent accidents and injuries. Helmets play a crucial role in protecting workers from head injuries caused by falling objects and machinery.  

Manual monitoring of helmet compliance is inefficient, error-prone, and unscalable. SafeGuard Corp sought to build an **automated computer vision system** that detects whether workers are wearing helmets, ensuring compliance, reducing risks, and improving safety enforcement .

---

## Objective
Develop an image classification model that can categorize images into:  
- **With Helmet**: Workers wearing safety helmets.  
- **Without Helmet**: Workers not wearing helmets.  

The ultimate goal is to **deploy this system for real-time monitoring** to ensure compliance with workplace safety regulations .

---

## Dataset
- **Size:** 631 labeled images.  
- **Balanced:** 311 ‚ÄúWith Helmet‚Äù, 320 ‚ÄúWithout Helmet‚Äù .  
- **Diversity:** Images span multiple environments (construction, factories, industrial sites) with variations in:  
  - Lighting conditions.  
  - Worker postures and activities.  
  - Angles and perspectives.  

This ensures robust learning for real-world deployment.  

---

## Key Observations (EDA)
- **Helmet vs. Non-Helmet cues:**  
  - ‚ÄúWithout Helmet‚Äù images often show hair clearly visible.
  - ‚ÄúWith Helmet‚Äù images feature distinct helmet shapes that stand out from caps or other headwear.

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>
  
- **Class balance:** Nearly 50-50, reducing the need for resampling.

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

- **Strong separability:** Visual differences between classes suggest feasibility for CNN-based classification .  

---

## Data Preprocessing (Expanded)
Before training, the image dataset was prepared carefully to ensure compatibility with deep learning models and robustness for real-world use cases:  

1. **Image Formatting & Dimensions**  
   - All images resized to **224√ó224 pixels** to align with the VGG16 model input size.  
   - Original RGB format retained, as helmets rely on color/texture cues that grayscale images may lose.
  
<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

2. **Normalization**  
   - Pixel intensity values scaled from **0‚Äì255 to 0‚Äì1**.  
   - This standardization improves training stability by ensuring smaller, comparable gradient updates.  

3. **Dataset Splitting**  
   - Training: **60%** of images  
   - Validation: **20%** of images  
   - Test: **20%** of images  
   - Stratified splits maintained the **helmet vs. no-helmet balance** in all subsets.  

4. **Augmentation (for CNN model)**  
   - Applied random rotations, flips, zooms, and shifts to increase dataset variability.  
   - Helped reduce overfitting, especially for the smaller basic CNN model.  

5. **Transfer Learning Prep**  
   - For VGG16, the **ImageNet normalization scheme** was applied (mean subtraction per channel).  
   - Base convolutional layers frozen initially; only the classifier head trained.  
   - Later, selective fine-tuning of deeper layers improved performance.  

 

---

## Modeling Approach (Expanded)

Two models were designed and benchmarked to evaluate the trade-offs between custom architectures and transfer learning:  

### 1. **Basic CNN (Custom Architecture)**  
- **Architecture:**  
  - 3 convolutional layers with ReLU activation and max-pooling.  
  - Flatten layer followed by dense fully connected layers.  
  - **Dropout (0.5)** applied to reduce overfitting.  
- **Optimizer & Loss:**  
  - Optimizer: Adam with learning rate = 0.001.  
  - Loss: Binary cross-entropy.  
- **Performance:**  
  - Served as a **baseline model**, achieving strong results (~97% test accuracy).  
  - However, a few misclassifications showed its limitations on edge cases (e.g., partially visible helmets).  

### 2. **Transfer Learning with VGG16 (HelmNet)**  
- **Architecture:**  
  - Pre-trained **VGG16** model used as a fixed feature extractor.

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

  - Convolutional base layers frozen to retain learned filters.  
  - Added custom dense layers (fully connected) for binary classification (helmet/no helmet).

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

- **Optimizer & Loss:**  
  - Optimizer: Adam, with fine-tuning on later layers after initial training.  
  - Loss: Binary cross-entropy.  
- **Performance:**  
  - Achieved **100% accuracy on both validation and test sets**.  
  - Zero false positives and zero false negatives ‚Äî perfect generalization on the dataset.  
- **Key Advantage:**  
  - Leveraged pre-trained ImageNet weights, enabling excellent performance despite limited dataset size (~631 images).  


---

## Evaluation Metrics (Expanded)

To evaluate model performance, multiple metrics were considered beyond simple accuracy:  

1. **Accuracy**  
   - Proportion of correctly classified images.  
   - With a balanced dataset (~50% helmets, ~50% no-helmets), accuracy was reliable.  
   - Final models achieved **99‚Äì100% accuracy**.  

2. **Precision**  
   - Fraction of predicted ‚Äúhelmet detected‚Äù cases that were correct.  
   - Important to avoid false positives (wrongly flagging helmets as present).  

3. **Recall (Sensitivity)**  
   - Fraction of actual ‚Äúhelmet present‚Äù cases that were correctly detected.  
   - Crucial to **avoid missing helmet violations** ‚Äî high recall ensures workers without helmets are flagged.  

4. **F1 Score**  
   - Harmonic mean of precision and recall.  
   - Ensures a balanced assessment ‚Äî especially critical in compliance tasks where both false negatives and false positives matter.  

### Why Accuracy *plus* Precision/Recall?  
- A model with high accuracy but poor recall could miss critical safety violations.  
- By combining **Accuracy + Precision + Recall + F1**, we ensured HelmNet achieves **zero tolerance for safety oversights**.  


---

## Model Results & Comparison (Full)

| Model                              | Train Accuracy | Validation Accuracy | Training Time (s) | Notes |
|-----------------------------------|----------------|----------------------|-------------------|-------|
| **Basic CNN**                     | 100%           | 99.21%               | ~20               | Lightweight, fast, but minor overfitting signs. |
| **CNN with VGG16 (Model 1)**      | 100%           | 100%                 | ~26               | Perfect performance; fewer trainable params due to frozen layers. |
| **CNN with VGG16 + FFNN (Model 2)** | 99.74%        | 98.41%               | ~16               | Larger trainable parameter count; slightly lower validation accuracy. |
| **CNN with VGG16 + Data Augmentation** | 100%        | 100%                 | ~27               | Perfect accuracy; more robust, but slower due to augmentation overhead. |

‚úÖ **Final Selection:** **CNN with VGG16 (Model 1)**  
- Delivered **perfect training & validation accuracy** with fewer parameters than Model 2.  
- Comparable training time to other models.  
- Achieved generalization without the need for augmentation.  
- Provides the **best trade-off between simplicity, speed, and accuracy**.

<p align="center">
  <img src="images/Renewind_logo.png" width="600"/>
</p>

---

## Actionable Insights & Business Impact (Expanded)

### Actionable Insights
1. **Deploy Transfer Learning (VGG16) Models:**  
   - Transfer learning outperforms custom CNNs, especially on smaller datasets.  
   - Highlights the effectiveness of leveraging pre-trained architectures for workplace safety solutions.  

2. **Automated Compliance Monitoring:**  
   - Model enables **real-time video feed integration** to automatically detect workers without helmets.  
   - Reduces dependency on manual supervisors, freeing up human resources.  

3. **Scalability:**  
   - The approach can be extended to detect other PPE (Personal Protective Equipment) such as safety vests, goggles, or gloves.  
   - Flexible enough for multi-class detection tasks in broader industrial safety monitoring.  

### Business Impact
- **100% Accuracy ‚Üí Zero Safety Oversights**  
  - Every worker without a helmet is detected, ensuring maximum safety compliance.  

- **Cost Savings on Safety Violations**  
  - Prevents potential fines and penalties for regulatory non-compliance.  
  - Reduces medical/legal costs by preventing head injuries.  

- **Operational Efficiency**  
  - Eliminates the need for continuous manual monitoring.  
  - Allows supervisors to focus on **higher-value tasks** while AI ensures safety compliance in the background.  

- **Reputation & Trust**  
  - Demonstrates commitment to worker safety, improving employee morale and corporate reputation.  

**Bottom Line:**  
HelmNet transforms workplace safety monitoring by ensuring **100% reliable helmet detection**. The perfect performance of the VGG16-based model makes it highly suitable for production deployment, enabling large-scale compliance enforcement and substantial safety improvements.  


